{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install mlxtend --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/translated_new_binned_DropNaN.csv\")\n",
    "print(f\"data.columns = {list(data.columns)}\")\n",
    "print()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "# Data preprocessing (same as before)\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return \"Young\"\n",
    "    elif 30 <= age < 60:\n",
    "        return \"Middle-aged\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif 18.5 <= bmi < 25:\n",
    "        return \"Normal\"\n",
    "    elif 25 <= bmi < 30:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obese\"\n",
    "\n",
    "def categorize_bsa(bsa):\n",
    "    if bsa < 1.5:\n",
    "        return \"Small\"\n",
    "    elif 1.5 <= bsa < 1.8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "# Apply categorizations\n",
    "data[\"Age_Category\"] = data[\"Age\"].apply(categorize_age)\n",
    "data[\"BMI_Category\"] = data[\"BMI\"].apply(categorize_bmi)\n",
    "data[\"BSA_Category\"] = data[\"BSA\"].apply(categorize_bsa)\n",
    "\n",
    "# Create binary columns for each category\n",
    "columns_to_encode = [\n",
    "    \"Age_Category\",\n",
    "    \"Sex\",\n",
    "    \"Smoke\",\n",
    "    \"BMI_Category\",\n",
    "    \"BSA_Category\",\n",
    "    \"occupation_category\",\n",
    "    # \"A_20th_quantile_binned\",\n",
    "    # \"A_25th_quantile_binned\",\n",
    "    # \"A_50th_quantile_binned\",\n",
    "    # \"A_75th_quantile_binned\",\n",
    "    # \"A_max_binned\",\n",
    "    # \"A_mean_binned\",\n",
    "    # \"A_median_binned\",\n",
    "    # \"A_min_binned\",\n",
    "    # \"A_std_binned\",\n",
    "    \"M_20th_quantile_binned\",\n",
    "    \"M_25th_quantile_binned\",\n",
    "    \"M_50th_quantile_binned\",\n",
    "    \"M_75th_quantile_binned\",\n",
    "    \"M_max_binned\",\n",
    "    \"M_mean_binned\",\n",
    "    \"M_median_binned\",\n",
    "    \"M_min_binned\",\n",
    "    \"M_std_binned\",\n",
    "]\n",
    "\n",
    "encoded_data = pd.get_dummies(data[columns_to_encode], prefix=columns_to_encode)\n",
    "\n",
    "# Convert to boolean type\n",
    "encoded_data = encoded_data.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_apriori(data, min_support, min_confidence, min_lift=1):\n",
    "    frequent_itemsets = apriori(\n",
    "        data, min_support=min_support, use_colnames=True, low_memory=True\n",
    "    )\n",
    "    rules = association_rules(\n",
    "        frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence\n",
    "    )\n",
    "    rules = rules[rules[\"lift\"] >= min_lift]\n",
    "    return rules.sort_values(\"lift\", ascending=False)\n",
    "\n",
    "\n",
    "def experiment_with_thresholds(data, support_thresholds, confidence_thresholds):\n",
    "    results = []\n",
    "    for support in support_thresholds:\n",
    "        for confidence in confidence_thresholds:\n",
    "            rules = run_apriori(data, support, confidence)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"min_support\": support,\n",
    "                    \"min_confidence\": confidence,\n",
    "                    \"num_rules\": len(rules),\n",
    "                    \"top_rules\": rules.head(5) if len(rules) > 0 else pd.DataFrame(),\n",
    "                }\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_apriori_in_batches(\n",
    "    data, support_thresholds, confidence_thresholds, batch_size=1000\n",
    "):\n",
    "    results = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data.iloc[i : i + batch_size]\n",
    "        batch_results = experiment_with_thresholds(\n",
    "            batch, support_thresholds, confidence_thresholds\n",
    "        )\n",
    "        results.extend(batch_results)\n",
    "        print(f\"Processed batch {i//batch_size + 1}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define ranges for support and confidence thresholds\n",
    "support_thresholds = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "confidence_thresholds = [0.5, 0.7, 0.75, 0.9]\n",
    "\n",
    "# # Run experiments\n",
    "# experiment_results = experiment_with_thresholds(\n",
    "#     encoded_data, support_thresholds, confidence_thresholds\n",
    "# )\n",
    "\n",
    "# Use the function\n",
    "batch_size = 5000  # Adjust this value based on your available memory\n",
    "experiment_results = run_apriori_in_batches(\n",
    "    encoded_data, support_thresholds, confidence_thresholds, batch_size\n",
    ")\n",
    "\n",
    "experiment_results_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "experiment_results_df.to_csv(\"apriori_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to summarize the results\n",
    "summary_data = []\n",
    "# Print results\n",
    "for result in experiment_results:\n",
    "    print(\n",
    "        f\"\\nMin Support: {result['min_support']}, Min Confidence: {result['min_confidence']}\"\n",
    "    )\n",
    "    print(f\"Number of rules generated: {result['num_rules']}\")\n",
    "    if not result[\"top_rules\"].empty:\n",
    "        print(\"Top 5 rules:\")\n",
    "        print(\n",
    "            result[\"top_rules\"][\n",
    "                [\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(\"No rules generated with these thresholds.\")\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"min_support\": result[\"min_support\"],\n",
    "            \"min_confidence\": result[\"min_confidence\"],\n",
    "            \"num_rules\": result[\"num_rules\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize number of rules for different thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "for confidence in confidence_thresholds:\n",
    "    data = summary_df[summary_df[\"min_confidence\"] == confidence]\n",
    "    plt.plot(\n",
    "        data[\"min_support\"],\n",
    "        data[\"num_rules\"],\n",
    "        marker=\"o\",\n",
    "        label=f\"Confidence = {confidence}\",\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Minimum Support\")\n",
    "plt.ylabel(\"Number of Rules\")\n",
    "plt.title(\"Number of Rules vs. Minimum Support for Different Confidence Levels\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(summary_df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "summary_df.to_csv(\"apriori_results_summary.csv\", index=False)\n",
    "\n",
    "print(\"Summary DataFrame has been created and saved to 'apriori_results_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "def analyze_rules(rules_df, min_lift=1, min_confidence=0.5, min_support=0.05):\n",
    "    \"\"\"\n",
    "    Analyze and find the best rules based on multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    rules_df: DataFrame containing association rules\n",
    "    min_lift: Minimum lift threshold\n",
    "    min_confidence: Minimum confidence threshold\n",
    "    min_support: Minimum support threshold\n",
    "    \"\"\"\n",
    "    # Filter rules based on thresholds\n",
    "    filtered_rules = rules_df[\n",
    "        (rules_df['lift'] >= min_lift) &\n",
    "        (rules_df['confidence'] >= min_confidence) &\n",
    "        (rules_df['support'] >= min_support)\n",
    "    ]\n",
    "    \n",
    "    # Sort rules by different metrics\n",
    "    best_by_lift = filtered_rules.nlargest(10, 'lift')\n",
    "    best_by_confidence = filtered_rules.nlargest(10, 'confidence')\n",
    "    best_by_support = filtered_rules.nlargest(10, 'support')\n",
    "    \n",
    "    # Add additional metrics if they don't exist\n",
    "    if 'conviction' not in filtered_rules.columns:\n",
    "        filtered_rules['conviction'] = np.where(\n",
    "            filtered_rules['confidence'] == 1,\n",
    "            np.inf,\n",
    "            (1 - filtered_rules['consequent support']) / (1 - filtered_rules['confidence'])\n",
    "        )\n",
    "    \n",
    "    if 'leverage' not in filtered_rules.columns:\n",
    "        filtered_rules['leverage'] = filtered_rules['support'] - (\n",
    "            filtered_rules['antecedent support'] * filtered_rules['consequent support']\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'filtered_rules': filtered_rules,\n",
    "        'best_by_lift': best_by_lift,\n",
    "        'best_by_confidence': best_by_confidence,\n",
    "        'best_by_support': best_by_support\n",
    "    }\n",
    "\n",
    "def visualize_rule_metrics(rules_df):\n",
    "    \"\"\"\n",
    "    Create visualizations to help identify the best rules.\n",
    "    \"\"\"\n",
    "    # Scatter plot with lift encoded in color and size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(rules_df['support'], \n",
    "                         rules_df['confidence'],\n",
    "                         c=rules_df['lift'],\n",
    "                         s=rules_df['lift']*50,\n",
    "                         cmap='viridis',\n",
    "                         alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Lift')\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Support vs Confidence (color and size indicate Lift)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    sns.histplot(rules_df['support'], bins=30, ax=axes[0])\n",
    "    axes[0].set_title('Distribution of Support')\n",
    "    \n",
    "    sns.histplot(rules_df['confidence'], bins=30, ax=axes[1])\n",
    "    axes[1].set_title('Distribution of Confidence')\n",
    "    \n",
    "    sns.histplot(rules_df['lift'], bins=30, ax=axes[2])\n",
    "    axes[2].set_title('Distribution of Lift')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_best_rules(rules_df, target_consequent=None, min_metrics=None):\n",
    "    \"\"\"\n",
    "    Find the best rules based on specific criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    rules_df: DataFrame containing association rules\n",
    "    target_consequent: Optional specific consequent to look for\n",
    "    min_metrics: Dictionary with minimum values for metrics\n",
    "    \"\"\"\n",
    "    if min_metrics is None:\n",
    "        min_metrics = {\n",
    "            'support': 0.05,\n",
    "            'confidence': 0.7,\n",
    "            'lift': 1.5\n",
    "        }\n",
    "    \n",
    "    filtered_rules = rules_df.copy()\n",
    "    \n",
    "    # Filter by metrics\n",
    "    for metric, threshold in min_metrics.items():\n",
    "        filtered_rules = filtered_rules[filtered_rules[metric] >= threshold]\n",
    "    \n",
    "    # Filter by target consequent if specified\n",
    "    if target_consequent:\n",
    "        filtered_rules = filtered_rules[\n",
    "            filtered_rules['consequents'].apply(lambda x: target_consequent in x)\n",
    "        ]\n",
    "    \n",
    "    # Calculate a composite score (normalized weighted sum)\n",
    "    filtered_rules['score'] = (\n",
    "        (filtered_rules['lift'] / filtered_rules['lift'].max()) * 0.4 +\n",
    "        (filtered_rules['confidence'] / filtered_rules['confidence'].max()) * 0.4 +\n",
    "        (filtered_rules['support'] / filtered_rules['support'].max()) * 0.2\n",
    "    )\n",
    "    \n",
    "    return filtered_rules.nlargest(10, 'score')\n",
    "\n",
    "def print_rule_summary(rules_dict):\n",
    "    \"\"\"\n",
    "    Print a summary of the best rules in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Best Rules by Lift ===\")\n",
    "    for _, rule in rules_dict['best_by_lift'].iterrows():\n",
    "        print(f\"\\nIf {list(rule['antecedents'])} then {list(rule['consequents'])}\")\n",
    "        print(f\"Lift: {rule['lift']:.2f}\")\n",
    "        print(f\"Confidence: {rule['confidence']:.2f}\")\n",
    "        print(f\"Support: {rule['support']:.2f}\")\n",
    "    \n",
    "    print(\"\\n=== Best Rules by Confidence ===\")\n",
    "    for _, rule in rules_dict['best_by_confidence'].iterrows():\n",
    "        print(f\"\\nIf {list(rule['antecedents'])} then {list(rule['consequents'])}\")\n",
    "        print(f\"Confidence: {rule['confidence']:.2f}\")\n",
    "        print(f\"Lift: {rule['lift']:.2f}\")\n",
    "        print(f\"Support: {rule['support']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rules\n",
    "rules = run_apriori(encoded_data, min_support=0.1, min_confidence=0.5)\n",
    "\n",
    "# Analyze rules\n",
    "analysis_results = analyze_rules(rules, min_lift=1.2, min_confidence=0.6, min_support=0.1)\n",
    "\n",
    "# Visualize the rules\n",
    "visualize_rule_metrics(rules)\n",
    "\n",
    "\"\"\"\n",
    "High lift: Shows strong association between items\n",
    "High confidence: Shows high reliability of the rule\n",
    "High support: Shows the rule applies to a significant portion of your dataset\n",
    "Balanced metrics: Good performance across all metrics\n",
    "\"\"\"\n",
    "\n",
    "# Find and print the best rules\n",
    "best_rules = find_best_rules(\n",
    "    rules,\n",
    "    min_metrics={'support': 0.1, 'confidence': 0.7, 'lift': 1.5}\n",
    ")\n",
    "\n",
    "# Print summary of the best rules\n",
    "print_rule_summary(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Key Observations:\n",
    "\n",
    "1. Strongest Rule Pattern:\n",
    "```\n",
    "If ['M_mean_binned_M_mean_Q2', 'M_20th_quantile_binned_M_20th_quantile_Q3'] \n",
    "then ['M_75th_quantile_binned_M_75th_quantile_Q2', 'Sex_F', 'M_25th_quantile_binned_M_25th_quantile_Q3']\n",
    "```\n",
    "- Lift: 8.60 (8.6 times more likely than random chance)\n",
    "- Confidence: 100% (occurs every time the antecedent is present)\n",
    "- Support: 10% (appears in 10% of all cases)\n",
    "\n",
    "2. Common Patterns:\n",
    "- Almost all strong rules involve:\n",
    "  - Female patients (Sex_F)\n",
    "  - Middle quartile measurements (Q2, Q3)\n",
    "  - Multiple statistical measures (mean, percentiles)\n",
    "\n",
    "3. Quality Metrics:\n",
    "- Lift values are consistently high (8.60)\n",
    "- Confidence levels are excellent (90-100%)\n",
    "- Support is stable at 10%\n",
    "\n",
    "4. Notable Relationships:\n",
    "- Mean values in Q2 (middle range) strongly predict:\n",
    "  - 75th percentile values in Q2\n",
    "  - 20th percentile values in Q3\n",
    "  - Female gender\n",
    "\n",
    "# Best Rules Selection:\n",
    "1. Most Reliable Rule:\n",
    "```\n",
    "If ['M_mean_binned_M_mean_Q2', 'M_std_binned_M_std_Q1', 'M_25th_quantile_binned_M_25th_quantile_Q3'] \n",
    "then ['M_75th_quantile_binned_M_75th_quantile_Q2', 'Sex_F', 'M_20th_quantile_binned_M_20th_quantile_Q3']\n",
    "```\n",
    "- Perfect confidence (1.0)\n",
    "- High lift (8.60)\n",
    "- Good support (0.10)\n",
    "\n",
    "2. Most Practical Rule (Simpler Antecedent):\n",
    "```\n",
    "If ['M_mean_binned_M_mean_Q2', 'M_20th_quantile_binned_M_20th_quantile_Q3'] \n",
    "then ['M_75th_quantile_binned_M_75th_quantile_Q2', 'Sex_F', 'M_25th_quantile_binned_M_25th_quantile_Q3']\n",
    "```\n",
    "- Same high metrics but fewer conditions to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class PatientClusterAnalyzer:\n",
    "    def __init__(self, data, rules):\n",
    "        self.data = data\n",
    "        self.rules = rules\n",
    "        self.clusters = None\n",
    "        self.cluster_profiles = None\n",
    "        \n",
    "    def create_rule_features(self):\n",
    "        \"\"\"Create features based on how many rules each patient satisfies\"\"\"\n",
    "        # Initialize dictionary to store rule matches\n",
    "        rule_matches = {}\n",
    "        \n",
    "        # Process all rules at once\n",
    "        for idx, rule in self.rules.iterrows():\n",
    "            antecedent_cols = list(rule['antecedents'])\n",
    "            consequent_cols = list(rule['consequents'])\n",
    "            \n",
    "            # Check if patient matches rule conditions\n",
    "            antecedent_match = self.data[antecedent_cols].all(axis=1)\n",
    "            consequent_match = self.data[consequent_cols].all(axis=1)\n",
    "            rule_matches[f'rule_{idx}'] = (antecedent_match & consequent_match).astype(int)\n",
    "        \n",
    "        # Create DataFrame all at once\n",
    "        rule_features = pd.DataFrame(rule_matches, index=self.data.index)\n",
    "        return rule_features\n",
    "    \n",
    "    def cluster_patients(self, n_clusters=None):\n",
    "        \"\"\"Cluster patients based on rule satisfaction patterns\"\"\"\n",
    "        # Create features based on rules\n",
    "        rule_features = self.create_rule_features()\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(rule_features)\n",
    "        \n",
    "        # Find optimal number of clusters if not specified\n",
    "        if n_clusters is None:\n",
    "            n_clusters = self.find_optimal_clusters(scaled_features)\n",
    "            print(f\"Optimal number of clusters determined: {n_clusters}\")\n",
    "            \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.clusters = kmeans.fit_predict(scaled_features)\n",
    "        \n",
    "        # Add cluster assignments to original data\n",
    "        self.data = self.data.copy()  # Create a copy to avoid fragmentation\n",
    "        self.data['Cluster'] = self.clusters\n",
    "        \n",
    "        # Create cluster profiles\n",
    "        self.cluster_profiles = self.create_cluster_profiles(rule_features)\n",
    "        \n",
    "        return self.clusters\n",
    "    \n",
    "    def find_optimal_clusters(self, features, max_clusters=10):\n",
    "        \"\"\"Find optimal number of clusters using silhouette score\"\"\"\n",
    "        silhouette_scores = []\n",
    "        print(\"Finding optimal number of clusters...\")\n",
    "        \n",
    "        for k in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(features)\n",
    "            silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            print(f\"Clusters: {k}, Silhouette Score: {silhouette_avg:.3f}\")\n",
    "            \n",
    "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "        return optimal_clusters\n",
    "    \n",
    "    def create_cluster_profiles(self, rule_features):\n",
    "        \"\"\"Create profiles for each cluster based on rule satisfaction\"\"\"\n",
    "        profiles = defaultdict(dict)\n",
    "        \n",
    "        # Calculate all cluster statistics at once\n",
    "        cluster_sizes = pd.Series(self.clusters).value_counts()\n",
    "        total_patients = len(self.data)\n",
    "        \n",
    "        for cluster in range(len(set(self.clusters))):\n",
    "            cluster_mask = self.clusters == cluster\n",
    "            cluster_data = self.data[cluster_mask]\n",
    "            \n",
    "            # Calculate rule satisfaction percentages\n",
    "            rule_satisfaction = rule_features[cluster_mask].mean()\n",
    "            \n",
    "            profiles[cluster] = {\n",
    "                'size': cluster_sizes[cluster],\n",
    "                'percentage': (cluster_sizes[cluster] / total_patients) * 100,\n",
    "                'top_rules': rule_satisfaction[rule_satisfaction > 0.5].index.tolist(),\n",
    "                'mean_rule_satisfaction': rule_satisfaction.mean(),\n",
    "                'common_characteristics': self._get_common_characteristics(cluster_data)\n",
    "            }\n",
    "            \n",
    "        return profiles\n",
    "    \n",
    "    def _get_common_characteristics(self, cluster_data, threshold=0.6):\n",
    "        \"\"\"Find common characteristics in a cluster\"\"\"\n",
    "        # Process all columns at once\n",
    "        characteristics = {}\n",
    "        numeric_cols = cluster_data.select_dtypes(include=['bool', 'int64']).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            value_counts = cluster_data[col].value_counts(normalize=True)\n",
    "            if len(value_counts) > 0 and value_counts.iloc[0] > threshold:\n",
    "                characteristics[col] = value_counts.index[0]\n",
    "                \n",
    "        return characteristics\n",
    "    \n",
    "    def visualize_clusters(self):\n",
    "        \"\"\"Visualize cluster characteristics\"\"\"\n",
    "        if self.clusters is None:\n",
    "            raise ValueError(\"Must run cluster_patients() first\")\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot cluster sizes\n",
    "        cluster_sizes = pd.Series(self.clusters).value_counts().sort_index()\n",
    "        cluster_sizes.plot(kind='bar', ax=ax1)\n",
    "        ax1.set_title('Cluster Sizes')\n",
    "        ax1.set_xlabel('Cluster')\n",
    "        ax1.set_ylabel('Number of Patients')\n",
    "        \n",
    "        # Plot rule satisfaction\n",
    "        rule_satisfaction = pd.Series({\n",
    "            i: self.cluster_profiles[i]['mean_rule_satisfaction'] \n",
    "            for i in range(len(self.cluster_profiles))\n",
    "        }).sort_index()\n",
    "        \n",
    "        rule_satisfaction.plot(kind='bar', ax=ax2)\n",
    "        ax2.set_title('Average Rule Satisfaction by Cluster')\n",
    "        ax2.set_xlabel('Cluster')\n",
    "        ax2.set_ylabel('Mean Rule Satisfaction')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional visualization: Rule satisfaction heatmap\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        rule_features = self.create_rule_features()\n",
    "        cluster_rule_satisfaction = pd.DataFrame({\n",
    "            f'Cluster {i}': rule_features[self.clusters == i].mean()\n",
    "            for i in range(len(self.cluster_profiles))\n",
    "        })\n",
    "        \n",
    "        plt.imshow(cluster_rule_satisfaction.T, aspect='auto', cmap='YlOrRd')\n",
    "        plt.colorbar(label='Rule Satisfaction Rate')\n",
    "        plt.xlabel('Rules')\n",
    "        plt.ylabel('Clusters')\n",
    "        plt.title('Rule Satisfaction Patterns by Cluster')\n",
    "        plt.show()\n",
    "        \n",
    "    def print_cluster_insights(self):\n",
    "        \"\"\"Print detailed insights about each cluster\"\"\"\n",
    "        print(\"\\n=== Cluster Analysis Results ===\")\n",
    "        print(f\"Total number of clusters: {len(self.cluster_profiles)}\")\n",
    "        print(f\"Total number of patients: {len(self.data)}\\n\")\n",
    "        \n",
    "        for cluster_id, profile in self.cluster_profiles.items():\n",
    "            print(f\"\\nCluster {cluster_id}:\")\n",
    "            print(f\"Size: {profile['size']} patients ({profile['percentage']:.1f}%)\")\n",
    "            \n",
    "            print(\"\\nCommon Characteristics:\")\n",
    "            chars = profile['common_characteristics']\n",
    "            if chars:\n",
    "                for char, value in chars.items():\n",
    "                    print(f\"- {char}: {value}\")\n",
    "            else:\n",
    "                print(\"- No strong characteristic patterns found\")\n",
    "                \n",
    "            print(\"\\nTop Rules Satisfied:\")\n",
    "            if profile['top_rules']:\n",
    "                for rule in profile['top_rules'][:5]:  # Show top 5 rules\n",
    "                    print(f\"- {rule}\")\n",
    "            else:\n",
    "                print(\"- No strong rule patterns found\")\n",
    "                \n",
    "            print(f\"\\nMean Rule Satisfaction: {profile['mean_rule_satisfaction']:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "def cluster_patients_from_rules(data, rules, n_clusters=None):\n",
    "    \"\"\"Main function to cluster patients based on association rules\"\"\"\n",
    "    print(\"Starting patient clustering analysis...\")\n",
    "    analyzer = PatientClusterAnalyzer(data, rules)\n",
    "    analyzer.cluster_patients(n_clusters)\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    analyzer.visualize_clusters()\n",
    "    print(\"\\nGenerating cluster insights...\")\n",
    "    analyzer.print_cluster_insights()\n",
    "    return analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using your existing data and rules\n",
    "analyzer = cluster_patients_from_rules(encoded_data, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install networkx --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# from matplotlib import collections as mc\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "# def print_detailed_rules(rules, top_n=10):\n",
    "#     print(f\"\\nTop {top_n} rules by lift:\")\n",
    "#     for i, (index, rule) in enumerate(rules.iterrows(), 1):\n",
    "#         if i > top_n:\n",
    "#             break\n",
    "#         antecedents = \", \".join(list(rule[\"antecedents\"]))\n",
    "#         consequents = \", \".join(list(rule[\"consequents\"]))\n",
    "#         print(f\"{i}. {antecedents} -> {consequents}\")\n",
    "#         print(f\"   Support: {rule['support']:.4f}\")\n",
    "#         print(f\"   Confidence: {rule['confidence']:.4f}\")\n",
    "#         print(f\"   Lift: {rule['lift']:.4f}\")\n",
    "#         print(f\"   Conviction: {rule['conviction']:.4f}\")\n",
    "#         print(f\"   Leverage: {rule['leverage']:.4f}\")\n",
    "#         print(f\"   Support Count: {int(rule['support'] * len(data))}\")\n",
    "#         print()\n",
    "\n",
    "\n",
    "# def visualize_top_rules(\n",
    "#     rules, top_n=10, figure_size=(20, 12), node_size_base=1000, font_size=8\n",
    "# ):\n",
    "#     G = nx.DiGraph()\n",
    "#     for i, (index, rule) in enumerate(rules.iterrows()):\n",
    "#         if i >= top_n:\n",
    "#             break\n",
    "#         for antecedent in rule[\"antecedents\"]:\n",
    "#             for consequent in rule[\"consequents\"]:\n",
    "#                 G.add_edge(\n",
    "#                     antecedent,\n",
    "#                     consequent,\n",
    "#                     weight=rule[\"lift\"],\n",
    "#                     support=rule[\"support\"],\n",
    "#                     confidence=rule[\"confidence\"],\n",
    "#                 )\n",
    "\n",
    "#     pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "\n",
    "#     degrees = dict(G.degree())\n",
    "#     node_sizes = [node_size_base * (1 + degrees[node]) for node in G.nodes()]\n",
    "\n",
    "#     edge_weights = [G[u][v][\"weight\"] for u, v in G.edges()]\n",
    "#     max_weight, min_weight = max(edge_weights), min(edge_weights)\n",
    "#     norm = mcolors.Normalize(vmin=min_weight, vmax=max_weight)\n",
    "#     edge_colors = plt.cm.viridis(norm(edge_weights))\n",
    "\n",
    "#     # Handle the case where all weights are the same\n",
    "#     if max_weight == min_weight:\n",
    "#         edge_widths = [1 for _ in edge_weights]\n",
    "#     else:\n",
    "#         edge_widths = [\n",
    "#             1 + 2 * (weight - min_weight) / (max_weight - min_weight)\n",
    "#             for weight in edge_weights\n",
    "#         ]\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=figure_size)\n",
    "\n",
    "#     # Draw edges with curved arrows\n",
    "#     curved_edges = [\n",
    "#         ((x1, y1), (x2, y2))\n",
    "#         for (x1, y1), (x2, y2) in (np.array([pos[u], pos[v]]) for u, v in G.edges())\n",
    "#     ]\n",
    "#     edge_collection = mc.LineCollection(\n",
    "#         curved_edges, colors=edge_colors, linewidths=edge_widths, alpha=0.7, zorder=1\n",
    "#     )\n",
    "#     ax.add_collection(edge_collection)\n",
    "\n",
    "#     # Draw arrow heads\n",
    "#     for (u, v), color, width in zip(G.edges(), edge_colors, edge_widths):\n",
    "#         x1, y1 = pos[u]\n",
    "#         x2, y2 = pos[v]\n",
    "#         dx, dy = x2 - x1, y2 - y1\n",
    "#         ax.arrow(\n",
    "#             x1,\n",
    "#             y1,\n",
    "#             dx * 0.8,\n",
    "#             dy * 0.8,\n",
    "#             color=color,\n",
    "#             width=width * 0.001,\n",
    "#             head_width=width * 0.005,\n",
    "#             head_length=width * 0.01,\n",
    "#             alpha=0.7,\n",
    "#             length_includes_head=True,\n",
    "#             zorder=2,\n",
    "#         )\n",
    "\n",
    "#         # Add edge labels\n",
    "#         edge_label = f\"L:{G[u][v]['weight']:.2f}\\nS:{G[u][v]['support']:.2f}\\nC:{G[u][v]['confidence']:.2f}\"\n",
    "#         ax.annotate(\n",
    "#             edge_label,\n",
    "#             xy=(x1 + dx * 0.4, y1 + dy * 0.4),\n",
    "#             xytext=(3, 3),\n",
    "#             textcoords=\"offset points\",\n",
    "#             fontsize=font_size - 2,\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.7),\n",
    "#             zorder=5,\n",
    "#         )\n",
    "\n",
    "#     # Draw nodes\n",
    "#     for node, (x, y) in pos.items():\n",
    "#         ax.scatter(\n",
    "#             x,\n",
    "#             y,\n",
    "#             s=node_sizes[list(G.nodes()).index(node)],\n",
    "#             c=\"lightblue\",\n",
    "#             alpha=0.8,\n",
    "#             zorder=3,\n",
    "#         )\n",
    "\n",
    "#     # Draw node labels with adjusted positions\n",
    "#     label_pos = {node: (coord[0], coord[1] + 0.02) for node, coord in pos.items()}\n",
    "#     for node, (x, y) in label_pos.items():\n",
    "#         ax.text(\n",
    "#             x,\n",
    "#             y,\n",
    "#             node,\n",
    "#             fontsize=font_size,\n",
    "#             fontweight=\"bold\",\n",
    "#             ha=\"center\",\n",
    "#             va=\"center\",\n",
    "#             zorder=4,\n",
    "#         )\n",
    "\n",
    "#     # Add a colorbar for edge weights\n",
    "#     sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "#     sm.set_array([])\n",
    "#     cbar = fig.colorbar(\n",
    "#         sm, ax=ax, label=\"Lift\", orientation=\"vertical\", fraction=0.046, pad=0.04\n",
    "#     )\n",
    "\n",
    "#     ax.set_title(\"Top Rules Network\", fontsize=16)\n",
    "#     ax.axis(\"off\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print detailed rule information\n",
    "#     print(\"\\nDetailed Rule Information:\")\n",
    "#     for i, (index, rule) in enumerate(rules.iterrows()):\n",
    "#         if i >= top_n:\n",
    "#             break\n",
    "#         print(f\"\\nRule {i+1}:\")\n",
    "#         print(f\"Antecedents: {', '.join(rule['antecedents'])}\")\n",
    "#         print(f\"Consequents: {', '.join(rule['consequents'])}\")\n",
    "#         print(f\"Support: {rule['support']:.4f}\")\n",
    "#         print(f\"Confidence: {rule['confidence']:.4f}\")\n",
    "#         print(f\"Lift: {rule['lift']:.4f}\")\n",
    "\n",
    "\n",
    "# def filter_rules_by_item(rules, item, in_antecedents=True, in_consequents=True):\n",
    "#     filtered_rules = rules[\n",
    "#         (rules[\"antecedents\"].apply(lambda x: item in x) if in_antecedents else True)\n",
    "#         | (rules[\"consequents\"].apply(lambda x: item in x) if in_consequents else True)\n",
    "#     ]\n",
    "#     return filtered_rules\n",
    "\n",
    "\n",
    "# # After running the Apriori algorithm\n",
    "# best_rules = run_apriori(encoded_data, min_support=0.1, min_confidence=0.5)\n",
    "\n",
    "# print_detailed_rules(best_rules)\n",
    "\n",
    "# # Example of filtering rules\n",
    "# asthma_related_rules = filter_rules_by_item(best_rules, \"BSA_Category_Large\")\n",
    "# print(\"\\nRules related to large BSA:\")\n",
    "# print_detailed_rules(asthma_related_rules)\n",
    "\n",
    "# # Visualize top rules\n",
    "# visualize_top_rules(\n",
    "#     best_rules, top_n=10, figure_size=(20, 10), node_size_base=100, font_size=10\n",
    "# )\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(\"\\nSummary Statistics:\")\n",
    "# print(f\"Total number of rules: {len(best_rules)}\")\n",
    "# print(f\"Average lift: {best_rules['lift'].mean():.4f}\")\n",
    "# print(f\"Average confidence: {best_rules['confidence'].mean():.4f}\")\n",
    "# print(f\"Average support: {best_rules['support'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
