{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3 - A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting script execution...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load the data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\"2024-Data-Cleaned/merged_patient_data.csv\")\n",
    "\n",
    "    # Print available columns\n",
    "    print(\"Available columns in the DataFrame:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Preprocess the data\n",
    "    print(\"Preprocessing data...\")\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"M\": 0, \"F\": 1})\n",
    "    df[\"Smoke\"] = df[\"Smoke\"].map({\"NS\": 0, \"ES\": 1})\n",
    "\n",
    "    # Select features for clustering\n",
    "    features = [\n",
    "        \"Age\",\n",
    "        \"Sex\",\n",
    "        \"Smoke\",\n",
    "        \"Smoke_amount\",\n",
    "        \"Height\",\n",
    "        \"Weight\",\n",
    "        \"BMI\",\n",
    "        \"BSA\",\n",
    "        \"Morning_PEFR\",\n",
    "        \"Afternoon_PEFR\",\n",
    "    ]\n",
    "    X = df[features]\n",
    "\n",
    "    # Handle missing values\n",
    "    print(\"Handling missing values...\")\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Normalize the features\n",
    "    print(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"n_clusters\": [2, 4, 5, 8],\n",
    "        \"affinity\": [\"rbf\", \"nearest_neighbors\"],\n",
    "        \"gamma\": [0.1, 1, 10],\n",
    "    }\n",
    "\n",
    "    # Perform manual grid search\n",
    "    print(\"Starting grid search...\")\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    total_combinations = len(list(ParameterGrid(param_grid)))\n",
    "\n",
    "    for i, params in enumerate(ParameterGrid(param_grid)):\n",
    "        print(f\"Testing combination {i+1}/{total_combinations}: {params}\")\n",
    "        spectral = SpectralClustering(**params, random_state=42, n_jobs=-1)\n",
    "        labels = spectral.fit_predict(X_scaled)\n",
    "        score = calinski_harabasz_score(X_scaled, labels)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "    print(\"Grid search completed.\")\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best Calinski-Harabasz score:\", best_score)\n",
    "\n",
    "    # Use the best parameters to perform the final clustering\n",
    "    print(\"Performing final clustering...\")\n",
    "    best_spectral = SpectralClustering(**best_params, random_state=42, n_jobs=-1)\n",
    "    df[\"Cluster\"] = best_spectral.fit_predict(X_scaled)\n",
    "\n",
    "    print(\"Clustering completed.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # version 2 - A\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer, calinski_harabasz_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv(\"2024-Data-Cleaned/merged_patient_data.csv\")\n",
    "\n",
    "# # Print available columns\n",
    "# print(\"Available columns in the DataFrame:\")\n",
    "# print(df.columns)\n",
    "\n",
    "# # Preprocess the data\n",
    "# df[\"Sex\"] = df[\"Sex\"].map({\"M\": 0, \"F\": 1})\n",
    "# df[\"Smoke\"] = df[\"Smoke\"].map({\"NS\": 0, \"ES\": 1})\n",
    "\n",
    "# # Select features for clustering\n",
    "# features = [\n",
    "#     \"Age\",\n",
    "#     \"Sex\",\n",
    "#     \"Smoke\",\n",
    "#     \"Smoke_amount\",\n",
    "#     \"Height\",\n",
    "#     \"Weight\",\n",
    "#     \"BMI\",\n",
    "#     \"BSA\",\n",
    "#     \"Morning_PEFR\",\n",
    "#     \"Afternoon_PEFR\",\n",
    "# ]\n",
    "# X = df[features]\n",
    "\n",
    "# # Handle missing values\n",
    "# imputer = SimpleImputer(strategy=\"mean\")\n",
    "# X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# # Normalize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     \"n_clusters\": [2, 4, 5, 8],\n",
    "#     \"affinity\": [\"rbf\", \"nearest_neighbors\"],\n",
    "#     # \"n_neighbors\": [5, 10, 15],\n",
    "#     \"gamma\": [0.1, 1, 10],\n",
    "# }\n",
    "\n",
    "# # Define a custom scorer (we'll use the Calinski-Harabasz Index)\n",
    "# ch_scorer = make_scorer(calinski_harabasz_score)\n",
    "\n",
    "# # Perform Grid Search\n",
    "# spectral = SpectralClustering(random_state=42)\n",
    "# grid_search = GridSearchCV(spectral, param_grid, scoring=ch_scorer, cv=5, n_jobs=-1)\n",
    "# grid_search.fit(X_scaled)\n",
    "\n",
    "# # Get the best parameters and score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best Calinski-Harabasz score:\", best_score)\n",
    "\n",
    "# # Use the best parameters to perform the final clustering\n",
    "# best_spectral = SpectralClustering(**best_params, random_state=42)\n",
    "# df[\"Cluster\"] = best_spectral.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "cluster_summary = df.groupby(\"Cluster\")[features].mean()\n",
    "print(\"\\nCluster Summary:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df[\"Cluster\"], cmap=\"viridis\")\n",
    "plt.title(\"Spectral Clustering Results (PCA)\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature distributions across clusters\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"Cluster\", y=feature, data=df)\n",
    "    plt.title(f\"{feature} Distribution Across Clusters\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze PEFR trends within clusters\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "for cluster in df[\"Cluster\"].unique():\n",
    "    cluster_data = df[df[\"Cluster\"] == cluster]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_data[\"Date\"], cluster_data[\"Morning_PEFR\"], label=\"Morning PEFR\")\n",
    "    plt.plot(\n",
    "        cluster_data[\"Date\"], cluster_data[\"Afternoon_PEFR\"], label=\"Afternoon PEFR\"\n",
    "    )\n",
    "    plt.title(f\"PEFR Trends for Cluster {cluster}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"PEFR\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation analysis within clusters\n",
    "for cluster in df[\"Cluster\"].unique():\n",
    "    cluster_data = df[df[\"Cluster\"] == cluster]\n",
    "    correlation = cluster_data[features].corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\n",
    "    plt.title(f\"Correlation Heatmap for Cluster {cluster}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
