{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, loguniform\n",
    "\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to extract time series features\n",
    "def extract_time_series_features(group):\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"Age\": group[\"Age\"].iloc[0],\n",
    "            \"Sex\": group[\"Sex\"].iloc[0],\n",
    "            \"Smoke\": group[\"Smoke\"].iloc[0],\n",
    "            \"Smoke_amount\": group[\"Smoke_amount\"].iloc[0],\n",
    "            \"Height\": group[\"Height\"].iloc[0],\n",
    "            \"Weight\": group[\"Weight\"].iloc[0],\n",
    "            \"BMI\": group[\"BMI\"].iloc[0],\n",
    "            \"BSA\": group[\"BSA\"].iloc[0],\n",
    "            \"Morning_PEFR_mean\": group[\"Morning_PEFR\"].mean(),\n",
    "            \"Morning_PEFR_std\": group[\"Morning_PEFR\"].std(),\n",
    "            \"Afternoon_PEFR_mean\": group[\"Afternoon_PEFR\"].mean(),\n",
    "            \"Afternoon_PEFR_std\": group[\"Afternoon_PEFR\"].std(),\n",
    "            \"PEFR_range\": (group[\"Afternoon_PEFR\"] - group[\"Morning_PEFR\"]).mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"M\": 0, \"F\": 1})\n",
    "    df[\"Smoke\"] = df[\"Smoke\"].map({\"NS\": 0, \"ES\": 1, \"SM\": 2})\n",
    "\n",
    "    # Apply log transformation to handle potential extreme values\n",
    "    numeric_cols = [\n",
    "        \"Age\",\n",
    "        \"Smoke_amount\",\n",
    "        \"Height\",\n",
    "        \"Weight\",\n",
    "        \"BMI\",\n",
    "        \"BSA\",\n",
    "        \"Morning_PEFR\",\n",
    "        \"Afternoon_PEFR\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "    return df.groupby(\"ID\").apply(extract_time_series_features).reset_index()\n",
    "\n",
    "\n",
    "# Define features for clustering\n",
    "numeric_features = [\n",
    "    \"Age\",\n",
    "    \"Smoke_amount\",\n",
    "    \"Height\",\n",
    "    \"Weight\",\n",
    "    \"BMI\",\n",
    "    \"BSA\",\n",
    "    \"Morning_PEFR_mean\",\n",
    "    \"Morning_PEFR_std\",\n",
    "    \"Afternoon_PEFR_mean\",\n",
    "    \"Afternoon_PEFR_std\",\n",
    "    \"PEFR_range\",\n",
    "]\n",
    "categorical_features = [\"Sex\", \"Smoke\"]\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_features),\n",
    "    ],\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "# Create the BIRCH model\n",
    "birch = Birch(n_clusters=None)\n",
    "\n",
    "\n",
    "# Create BIRCH clustering pipeline\n",
    "birch_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"birch\", birch)])\n",
    "\n",
    "\n",
    "# Define a comprehensive scoring function\n",
    "def clustering_score(estimator, X):\n",
    "    preprocessed_X = estimator.named_steps[\"preprocessor\"].transform(X)\n",
    "    labels = estimator.named_steps[\"birch\"].fit_predict(preprocessed_X)\n",
    "    n_clusters = len(np.unique(labels))\n",
    "\n",
    "    if n_clusters == 1:\n",
    "        return 0  # Return 0 instead of -np.inf for single cluster case\n",
    "\n",
    "    try:\n",
    "        sil_score = silhouette_score(preprocessed_X, labels)\n",
    "        ch_score = calinski_harabasz_score(preprocessed_X, labels)\n",
    "        return (0.7 * sil_score) + (0.3 * ch_score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in clustering_score: {e}\")\n",
    "        return 0  # Return 0 instead of -np.inf\n",
    "\n",
    "\n",
    "# Define parameter grid for BIRCH\n",
    "param_grid = {\n",
    "    \"birch__threshold\": uniform(0.1, 2.0),\n",
    "    \"birch__branching_factor\": np.arange(10, 100),\n",
    "    \"birch__n_clusters\": [None] + list(range(2, 21)),  # None and 2 to 20 clusters\n",
    "}\n",
    "\n",
    "\n",
    "# liming clusters to max 10 or as mentioned\n",
    "def limit_clusters(labels, max_clusters=10):\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) > max_clusters:\n",
    "        # Merge smaller clusters\n",
    "        label_counts = np.bincount(labels)\n",
    "        top_labels = np.argsort(label_counts)[-max_clusters:]\n",
    "        new_labels = np.zeros_like(labels)\n",
    "        for i, label in enumerate(top_labels):\n",
    "            new_labels[labels == label] = i\n",
    "        return new_labels\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "def perform_grid_search(birch_pipeline, param_grid, X):\n",
    "\n",
    "    # First, use RandomizedSearchCV to explore a wide range of parameters\n",
    "    random_search = RandomizedSearchCV(\n",
    "        birch_pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=100,  # Number of parameter settings sampled\n",
    "        scoring=clustering_score,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV\n",
    "    random_search.fit(X)  # X is your preprocessed data\n",
    "\n",
    "    # Get the best parameters from RandomizedSearchCV\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    # Define a focused parameter grid based on the best results from RandomizedSearchCV\n",
    "    focused_param_grid = {\n",
    "        \"birch__branching_factor\": range(\n",
    "            max(10, best_params.get(\"branching_factor\", 50) - 10),\n",
    "            min(100, best_params.get(\"branching_factor\", 50) + 11),\n",
    "            5,\n",
    "        ),\n",
    "        \"birch__n_clusters\": [None]\n",
    "        + list(\n",
    "            range(\n",
    "                max(\n",
    "                    2,\n",
    "                    (\n",
    "                        best_params.get(\"n_clusters\", 5) - 2\n",
    "                        if best_params.get(\"n_clusters\")\n",
    "                        else 2\n",
    "                    ),\n",
    "                ),\n",
    "                min(\n",
    "                    20,\n",
    "                    (\n",
    "                        best_params.get(\"n_clusters\", 5) + 3\n",
    "                        if best_params.get(\"n_clusters\")\n",
    "                        else 5\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        birch_pipeline,\n",
    "        param_grid=focused_param_grid,\n",
    "        scoring=clustering_score,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "    )\n",
    "    grid_search.fit(X)\n",
    "\n",
    "    # Get the best model and limit the number of clusters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    labels = best_model.named_steps[\"birch\"].labels_\n",
    "    limited_labels = limit_clusters(labels, 20)\n",
    "    best_model.named_steps[\"birch\"].labels_ = limited_labels\n",
    "\n",
    "    # Use the best model for final clustering\n",
    "    final_labels = best_model.fit_predict(X)\n",
    "\n",
    "    print(f\"final labels : {final_labels}\")\n",
    "\n",
    "    return grid_search, best_model\n",
    "\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "def visualize_clusters(X, labels, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap=\"viridis\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"First Principal Component\")\n",
    "    plt.ylabel(\"Second Principal Component\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "def analyze_feature_importance(feature_names, subcluster_centers):\n",
    "    importances = np.abs(subcluster_centers).mean(axis=0)\n",
    "\n",
    "    # Ensure feature_names and importances have the same length\n",
    "    if len(feature_names) != len(importances):\n",
    "        print(\n",
    "            f\"Warning: Mismatch in feature names ({len(feature_names)}) and importance scores ({len(importances)})\"\n",
    "        )\n",
    "        # Truncate the longer list to match the shorter one\n",
    "        min_length = min(len(feature_names), len(importances))\n",
    "        feature_names = feature_names[:min_length]\n",
    "        importances = importances[:min_length]\n",
    "\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"feature\": feature_names, \"importance\": importances}\n",
    "    )\n",
    "    feature_importance = feature_importance.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance)\n",
    "    plt.title(\"Feature Importance in Clustering\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "\n",
    "# Print cluster characteristics\n",
    "def print_cluster_characteristics(df, labels):\n",
    "    df_with_clusters = df.copy()\n",
    "    df_with_clusters[\"Cluster\"] = labels\n",
    "    for cluster in range(len(np.unique(labels))):\n",
    "        print(f\"\\nCluster {cluster} characteristics:\")\n",
    "        cluster_data = df_with_clusters[df_with_clusters[\"Cluster\"] == cluster]\n",
    "        for feature in numeric_features + categorical_features:\n",
    "            if feature in numeric_features:\n",
    "                print(\n",
    "                    f\"{feature}: mean = {cluster_data[feature].mean():.2f}, std = {cluster_data[feature].std():.2f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{feature}:\")\n",
    "                print(cluster_data[feature].value_counts(normalize=True))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = load_data(\"2024-Data-Cleaned/merged_patient_data.csv\")\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    df_features = preprocess_data(df)\n",
    "\n",
    "    # Perform grid search\n",
    "    print(\"Performing BIRCH clustering grid search...\")\n",
    "    grid_search, best_model = perform_grid_search(\n",
    "        birch_pipeline, param_grid, df_features\n",
    "    )\n",
    "\n",
    "    # Print best parameters and score\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = best_model.named_steps[\"birch\"].labels_\n",
    "\n",
    "    # Check for single cluster case\n",
    "    if len(np.unique(cluster_labels)) < 2:\n",
    "        print(\n",
    "            \"Warning: Only one cluster found. Clustering may not be effective for this data.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    print(pd.Series(cluster_labels).value_counts())\n",
    "\n",
    "    # Visualize clusters\n",
    "    preprocessed_X = best_model.named_steps[\"preprocessor\"].transform(df_features)\n",
    "    visualize_clusters(\n",
    "        preprocessed_X, cluster_labels, \"Patient Clusters Visualization (PCA)\"\n",
    "    )\n",
    "\n",
    "    # Get feature names after preprocessing\n",
    "    numeric_transformer = best_model.named_steps[\"preprocessor\"].named_transformers_[\n",
    "        \"num\"\n",
    "    ]\n",
    "    categorical_transformer = best_model.named_steps[\n",
    "        \"preprocessor\"\n",
    "    ].named_transformers_[\"cat\"]\n",
    "    feature_names = (\n",
    "        numeric_features\n",
    "        + best_model.named_steps[\"preprocessor\"]\n",
    "        .named_transformers_[\"cat\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    subcluster_centers = best_model.named_steps[\"birch\"].subcluster_centers_\n",
    "    feature_importance = analyze_feature_importance(feature_names, subcluster_centers)\n",
    "\n",
    "    # Print cluster characteristics\n",
    "    print_cluster_characteristics(df_features, cluster_labels)\n",
    "\n",
    "    # Save results\n",
    "    np.save(\"Results/cluster_birch_labels.npy\", cluster_labels)\n",
    "    feature_importance.to_csv(\n",
    "        \"Results/cluster_birch_feature_importance.csv\", index=False\n",
    "    )\n",
    "    print(\"Results saved to cluster_labels.npy and feature_importance.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
